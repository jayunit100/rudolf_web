<h1>Parsing Beagle:  the code</h1>

<hr>
<h3>Parsing</h3>

<p>In the first part of this article, we covered the basic strategy of parsing:</p>

<ol>
<li><p>tokenizing a string</p></li>
<li><p>inspecting the Tokens, and removing some such as whitespace and comments</p></li>
<li><p>assembling the tokens into a parse tree</p></li>
</ol><p>Also, at each stage, appropriate error checking and handling actions must be taken.</p>

<h3>Stage 1: Tokenization</h3>

<p>Tokenization, also known as lexing, is the process of breaking a string into 
tokens, which are the syntactic atoms of a programming language. <em>(As mentioned in Part 1, 
Beagle defines 6 tokens: whitespace, comment, open, close, string, and symbol)</em>.</p>

<p>So we need a function of type <code>String -&gt; [Token]</code>:</p>

<pre class="code"><code>function tokenize(string) {
    var tokens = [],
        next;
    while (next = nextToken(string)) {
        tokens.push(next.token);
        string = next.rest;
    }
    return tokens;
}
</code></pre>

<p>This function loops through the input, extracting tokens, until the input is empty.  Note that 
whenever a token is found, not only is the token itself returned (<code>next.token</code>), but also the
rest of the string after the token (<code>next.rest</code>).</p>

<p>Of course, <code>tokenize</code> depends on <code>nextToken :: String -&gt; Maybe TokenRest</code>.  <code>nextToken</code>, given 
a string, needs to extract the next token and also return the rest of the string that the token
didn't consume.  It also has to do something sane if the string is empty (<code>return false;</code>).  It
doesn't expect to not be able to find a token -- if this happens and the string isn't empty, it
freaks out and throws an exception (this is assumed to be a bug if this happens).</p>

<pre class="code"><code>function nextToken(string) {
  var match;

  // 0. empty string
  if( string === "" ) {
    return false;
  } 

  // 1. leading whitespace
  if( match = string.match(WHITESPACE) ) {
    return {
      'token': new Token('whitespace', match[0]),
      'rest': string.substring(match[0].length)
    };
  }

  // 2. first char is '('
  if( string[0] === OPEN ) {
    return {
      'token': new Token('open', string[0]),
      'rest' : string.substring(1)
    };
  } 

  // 3. first char is ')'
  if( string[0] === CLOSE ) {
    return {
      'token': new Token('close', string[0]),
      'rest' : string.substring(1)
    };
  } 

  // 4. comment
  if( match = string.match(COMMENT) ) {
    return {
      'token': new Token('comment', match[1]),
      'rest' : string.substring(match[0].length)
    };
  }

  // 5. string
  if( string[0] === '"' ) {
    match = string.match(STRING);
    if( match ) {
      return {
        'token': new Token('string', match[1]),
        'rest': string.substring(match[0].length)
      };
    } else {
      throw new ParseError("tokenizer error: end-of-string (\") not found", string);
    }
  }

  // 6. symbol
  match = string.match(SYMBOL);
  if( match ) {
    return {
      'token': new Token('symbol', match[0]),
      'rest' : string.substring(match[0].length)
    };
  }

  throw new ParseError("unexpected tokenizer error:  no tokens match string", string);
}
</code></pre>

<p>Basically, <code>nextToken</code> just tries to match the beginning of the string to each of the
different token definitions in succession, and returns the match and the rest of the
string when it finds one that works.</p>

<h3>Stage 2: get rid of unwanted tokens</h3>

<p>Getting rid of whitespace and comment tokens is easy, especially since Javascript
arrays have a <code>filter :: [a] -&gt; (a -&gt; Boolean) -&gt; [a]</code> method:</p>

<pre class="code"><code>function stripTokens(tokens) {
  function isNotCommentNorWS(token) {
    return (token.type !== 'comment' &amp;&amp; token.type !== 'whitespace');
  }
  return tokens.filter(isNotCommentNorWS);
}
</code></pre>

<h3>Stage 3: Syntactic analysis</h3>

<p>In this stage, we have to assemble our tokens into a parse tree.  We'll call the result an
s-expression.</p>

<p>One of the great things about Lisp is its simple syntax:  s-expressions are either atoms,
which are strings and symbols, or lists, which are composed of s-expressions surrounded
by <code>(</code> and <code>)</code>.</p>

<p>So given a token stream, we can extract either an atom or a list; given appropriate 
definitions of <code>getAtom</code> and <code>getList</code>, this is what <code>getSExpression :: Maybe [Token] -&gt; SExpression</code>
looks like:</p>

<pre class="code"><code>function getSExpression(tokens) {
    var sexpr;

    if (tokens.length === 0) {
        return false;
    }

    // an s-expression is either an atom
    sexpr = getAtom(tokens);
    if (sexpr) {
        return sexpr;
    }

    // or a list
    sexpr = getList(tokens);
    if (sexpr) {
        return sexpr;
    }

    // no other possibilities
    throw new ParseError("unexpected error: couldn't find s-expression and token stream was not empty", tokens);
}
</code></pre>

<p>Note the boundary condition:  if the token stream is empty, clearly we can't find an
s-expression.  Then we try to extract an atom; if that doesn't work, we try to extract
a list.  Then there's a sanity check:  if we made a mistake in the implementation,
and somehow don't find <em>anything</em>, we'll get a nice exception instead of a silent
failure.</p>

<p>So how is <code>getAtom</code> implemented?</p>

<pre class="code"><code>// [Token] -&gt; Maybe SExpression
// returns false if token stream is empty or first token is NOT a symbol or string
function getAtom(tokens) {
    if (tokens.length === 0) {
        return false;
    }

    var first = tokens[0];

    if (first.type === 'symbol' || first.type === 'string') {
        return {
            result: new SExpression(first.type, first.value),
            rest: tokens.slice(1)
        };
    }

    return false;
}
</code></pre>

<p><code>getAtom</code> succeeds if it finds a symbol or a string as the next token, 
otherwise it returns false.</p>

<p>And here's <code>getList</code>:</p>

<pre class="code"><code>// [Token] -&gt; Maybe SExpression
// returns false if tokens is empty or doesn't start with open
// throws an error if first token is a ')'
// throws an error if a properly 'balanced' list can't be found
function getList(tokens) {
    var sexpr, elems = [],
        inputTokens = tokens;

    if (tokens.length === 0) {
        return false;
    }

    if (tokens[0].type === 'close') {
        throw new ParseError("')' token found without matching '('", inputTokens);
    }

    // a list *has* to start with a '('
    if (tokens[0].type !== 'open') {
        return false;
    }

    tokens = tokens.slice(1);

    // keep going until a ')'
    while (tokens[0] &amp;&amp; (tokens[0].type !== 'close')) {
        // a list could have as many nested lists or atoms as it pleased
        sexpr = getSExpression(tokens);
        if (!sexpr) {
            return false;
        }
        elems.push(sexpr.result);
        tokens = sexpr.rest;
    }

    // a list needs a close-paren
    if (tokens[0] &amp;&amp; tokens[0].type === 'close') {
        return {
            result: new SExpression('list', elems),
            rest: tokens.slice(1)
        };
    }

    // uh-oh! we didn't find a close-paren ...
    throw new ParseError("'(' token found without matching ')'", inputTokens);
}
</code></pre>

<p>That's more complicated because it has to check lots of error conditions and generate
helpful error message if it does detect any problems:</p>

<ul>
<li><p>a leading <code>)</code> would mean unbalanced parentheses</p></li>
<li><p>empty input, or input that doesn't start with <code>(</code>, isn't a list</p></li>
<li><p>we have to find a <code>)</code> somewhere ... if not, this would also be unbalanced!</p></li>
</ul><p>Besides all the error-handling, the main body of <code>getList</code> churns through the tokens,
finding more s-expressions until it hits a close-paren.</p>

<h3>Summary</h3>

<p>Vanilla recursive-descent parsers of simple languages are relatively straightforward
to code.  You just have to be clear and unambiguous in your token and parse tree
definitions, and write a lot of code for dealing with errors.</p>

<p>Once you've written a parser or two, you may come to realize that most of the code 
deals with error checking.  Check out a parser generator or a parser combinator 
library, which allows you to specify your grammar and generates most of the plumbing
for you!</p>
